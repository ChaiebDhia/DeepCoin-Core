# DeepCoin-Core â€” Copilot Persistent Context
# ============================================
# This file is automatically injected into every GitHub Copilot Chat session.
# It gives Copilot full knowledge of the project state, decisions, and rules.
# NEVER delete this file. Update it after every major milestone.
# Last updated: February 27, 2026 â€” Enterprise RAG upgrade COMPLETE (STEPs 0-8 done). Layer 3 fully production-ready.

---

## 0. IRON RULES â€” READ THESE FIRST, NEVER VIOLATE THEM

1. **"Never go to the next layer unless all is engineered as experts will do â€” enterprise-grade and production-ready."**
2. **"Don't add any code unless we discuss it first."** Always present the plan, wait for "go" approval.
3. **"Explain everything like teaching â€” WHAT it does, WHY it's designed this way, HOW it fits."**
4. Every function must have detailed docstrings.
5. This is a PFE (Final Year Engineering Internship) â€” ESPRIT School of Engineering Ã— YEBNI, Tunisia.
6. Student: **Dhia Chaieb** | GitHub: `ChaiebDhia` | Email: `dhia.chaieb@esprit.tn`
7. GitHub repo: `https://github.com/ChaiebDhia/DeepCoin-Core` | Branch: `main`
8. Always assume the Python venv at `C:\Users\Administrator\deepcoin\venv\` is active.
9. OS: Windows 11 | Shell: PowerShell 5.1 | Use `;` not `&&` to chain commands.
10. GPU: NVIDIA RTX 3050 Ti, 4.3 GB VRAM | CUDA 12.4 | PyTorch 2.6.0+cu124

---

## 1. PROJECT MISSION

Build an end-to-end industrial AI system that:
- Classifies degraded archaeological ancient coins from a photograph
- Routes the analysis through specialist AI agents based on confidence
- Returns a professional PDF report with historical narrative, forensic validation, and visual attributes
- Handles unknown coins gracefully (never returns "I don't know" â€” always returns useful output)
- Covers the full Corpus Nummorum domain (9,716 coin types in KB, 438 in CNN)

**The core philosophy:** "Failing gracefully is better than failing confidently."

---

## 2. COMPLETE PROJECT HISTORY â€” FROM RAW DATA TO NOW

This is the full chronological record. Every phase, every problem, every fix.

---

### PHASE 0 â€” Environment Setup (early February 2026) âœ…

**What we did:**
- Created `C:\Users\Administrator\deepcoin\` directory structure
- Initialized Python 3.11 virtual environment at `venv\`
- Set up Git repo: `https://github.com/ChaiebDhia/DeepCoin-Core`
- Created `.gitignore` (excludes: `data/`, `models/`, `venv/`, `.env`, `notes.md`, `The Project.md`)
- Created `requirements.txt` (50+ deps), professional `README.md`, `.gitkeep` files

**Problems:** None. Clean setup.

---

### PHASE 1 â€” Dataset Auditing (mid February 2026) âœ…

**Tool:** `src/data_pipeline/auditor.py`

**Discovery â€” Long-tail distribution problem:**
```
Raw dataset: 115,160 images across 9,716 coin types (folders in data/raw/)
Most types have only 1â€“3 images â†’ neural network cannot learn from that
Decision: apply â‰¥10 images per class threshold
Result: 9,716 types â†’ 438 viable classes, 7,677 images retained
```

Why â‰¥10 is the right cutoff: Transfer learning (ImageNet pretrained) reduces minimum data need from ~1,000 to ~10 images/class. Below 10, the model memorises rather than generalises.

---

### PHASE 1b â€” Preprocessing Engine (mid February 2026) âœ…

**File:** `src/data_pipeline/prep_engine.py`

**Step 1 â€” CLAHE in LAB colour space:**
- Convert BGR â†’ LAB (separates luminance L from colour channels A, B)
- Apply CLAHE to L channel only: `clipLimit=2.0, tileGridSize=(8,8)`
- Convert back to BGR
- Why LAB not RGB: RGB CLAHE distorts metal patina colours (the green/brown oxidation proving archaeological authenticity). LAB preserves colours while enhancing contrast on the luminance channel.

**Step 2 â€” Aspect-preserving resize to 299Ã—299:**
- Scale so longest edge = 299; use `INTER_AREA` (downscale) or `INTER_CUBIC` (upscale)
- Pad shorter edge with black zeros to reach 299Ã—299
- Why not simple resize: stretch deforms coin geometry. The model must learn coins are round.

**Output:** `data/processed/[class_id]/[files]` â€” 7,677 images, 438 class folders.

---

### PHASE 2 â€” Dataset Class (February 20, 2026) âœ…

**File:** `src/core/dataset.py` (248 lines)

`DeepCoinDataset(Dataset)` â€” PyTorch bridge between disk and training loop.
- Lazy loading: stores `(path, label)` tuples â€” NOT pixel arrays. Loading 7,677 images raw = 2.6 GB RAM. Lazy loading = one batch at a time = feasible.
- `class_to_idx`: maps folder name to integer (`"1015" â†’ 0`). Neural networks only understand numbers.
- `get_train_transforms()`: 6 Albumentations augmentations + ImageNet normalisation
- `get_val_transforms()`: normalise only (honest evaluation â€” no augmentation)

**Augmentations:**
```python
A.Rotate(limit=15, p=0.5)                        # tilted photos
A.RandomBrightnessContrast(0.2, 0.2, p=0.5)      # lighting variation
A.GaussNoise(p=0.3)                               # low-quality cameras
A.ElasticTransform(p=0.3)                         # worn/bent coins
A.HorizontalFlip(p=0.5)                           # either orientation
A.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])  # ImageNet stats â€” MANDATORY
```
ImageNet normalisation is MANDATORY. EfficientNet-B3 was pretrained with these exact stats. Wrong values â†’ pretrained features activate incorrectly â†’ ~15-20% accuracy loss.

**Critical discovery from `scripts/test_dataset.py`:**
```
Min images per class:  5  (class 5181)
Max images per class: 204 (class 3987)
Imbalance ratio:      40:1  â† must be corrected during training
```

---

### PHASE 3 â€” Model Architecture (February 2026) âœ…

**File:** `src/core/model_factory.py`

`get_deepcoin_model(num_classes=438, dropout=0.4)`:
- Base: `torchvision.models.efficientnet_b3(pretrained=True)` â€” ImageNet weights
- Replace head: `nn.Linear(1536, 1000)` â†’ `nn.Sequential(nn.Dropout(0.4), nn.Linear(1536, 438))`
- Dropout 0.4: 40% of neurons zeroed per forward pass â†’ cannot rely on any single neuron â†’ less memorisation

Why EfficientNet-B3: compound scaling (depth + width + resolution simultaneously). B3 = best accuracy/parameter ratio for 4.3 GB VRAM. B7 would need ~8 GB.

The 1536-dim vector before the head = coin's "fingerprint" â€” 18 convolution layers encoding all visual features.

---

### PHASE 4 â€” Training V3 (February 2026) âœ…

**File:** `scripts/train.py` (729 lines)

```python
optimizer     = AdamW(lr=1e-4, weight_decay=0.01)
scheduler     = CosineAnnealingLR(T_max=100, eta_min=1e-6)
loss          = CrossEntropyLoss(label_smoothing=0.1)
augmentation  = Albumentations (6 transforms)
mixup_alpha   = 0.2        # Beta(0.2,0.2) blending
amp           = GradScaler('cuda') + autocast('cuda')   # halves VRAM
gradient_clip = max_norm=1.0
batch_size    = 16         # 4.3 GB VRAM constraint
early_stop    = patience=10 on val accuracy
seed          = 42
```

Mixup: `mixed = Î»Ã—A + (1-Î»)Ã—B` with `Î» ~ Beta(0.2,0.2)`. Smooth decision boundaries. Reduces train/val gap by ~3-4% on small datasets.

AMP: float16 gradients â†’ halves VRAM, ~2Ã— faster/epoch. GradScaler prevents underflow that would corrupt float16.

WeightedRandomSampler: weight_i = 1/count(class_i) â†’ each class seen approximately equally â†’ fixes 40:1 imbalance.

**Data split (stratified, seed=42):**
```
Train:      5,374  (70%)  â€” sampler applied
Validation: 1,151  (15%)  â€” no augmentation
Test:       1,152  (15%)  â€” run ONCE at end
```

**Results:**
```
Best epoch:         52 / 100
Val accuracy:       79.25%
Test accuracy:      79.08%  (single-pass)
TTA accuracy (Ã—8):  80.03%  â† official result
Macro F1:           0.7763  (438 classes)
Top confusion:      3314 â†’ 3987  (10Ã— misclassification)
Training time:      ~103 min on RTX 3050 Ti
Early stop:         epoch 62 (10 epochs no improvement)
```

---

### PHASE 4b â€” TTA Evaluation (February 2026) âœ…

**File:** `scripts/evaluate_tta.py`

TTA (Test-Time Augmentation): 8 forward passes per coin, averaged softmax:
```
Pass 1: original
Pass 2: horizontal flip
Pass 3: vertical flip
Pass 4: both flips
Pass 5-8: four 85% corner crops
```
Same coin, 8 orientations â†’ averaged prediction reduces noise â†’ +0.78% gain.

**Saved artefacts:**
```
models/best_model.pth          â† V3, epoch 52 â€” THE REAL MODEL
models/best_model_v1_80pct.pth â† MISLEADING NAME. Epoch 3, val 21.33%. NOT the 80% model. Ignore.
models/class_mapping.pth       â† {class_to_idx, idx_to_class, n:438}
```

---

### PHASE 5 â€” Inference Engine (February 2026) âœ…

**Files:** `src/core/inference.py`, `scripts/predict.py`

`CoinInference` â€” production wrapper:
- `predict(image_path, tta=False)` â†’ `{class_id, label, confidence, top5, tta_used}`

**Bug found and fixed (Bug #2 â€” see Section 12):**
```
"auto" string passed directly to model.to("auto") â†’ RuntimeError
Fix: resolve before passing: device = "cuda" if torch.cuda.is_available() else "cpu"
```

---

### PHASE 6 â€” Knowledge Base (February 2026) âœ… (needs upgrade)

**Files:** `src/core/knowledge_base.py` (343 lines), `scripts/build_knowledge_base.py` (296 lines)

**Scraper:** Fetches `https://www.corpus-nummorum.eu/types/{id}` at 1 req/sec. Parses `<dl>` blocks â†’ 15 structured fields. Saves every 50 types (crash-safe). SSL verification disabled (lab env).

**KB state:**
- ChromaDB `PersistentClient` at `data/metadata/chroma_db/`
- Collection `cn_coin_types`, embedding `all-MiniLM-L6-v2` (384-dim, cosine, CPU, 22 MB)
- 434 documents (4 types returned HTTP errors â†’ filtered)
- Document format: one 200-word text blob per coin type

**Scraper bugs found and fixed (see Section 11 for details):**
- SSL certificate errors â†’ disabled cert verification
- Emoji/navigation chars in scraped HTML â†’ regex cleanup
- Mint field contained "Region:" suffix â†’ regex split
- 4/438 types returned HTTP errors â†’ error records filtered in `build_from_metadata()`

**API:**
```python
kb.search(query, n, where)   # cosine similarity: 1.0 - distance
kb.search_by_id(type_id)     # exact ID lookup via ChromaDB .get()
kb.build_from_metadata(path) # batch upsert (50/batch)
get_knowledge_base()         # module-level singleton
```

**BEFORE the upgrade â€” what the KB is today:**
- 438 coin types only (the CNN training subset â€” 4.5% of the full CN domain)
- Each coin stored as ONE 200-word text blob: all fields concatenated into a paragraph
- ChromaDB encodes that blob into one 384-dim vector
- When the Historian needs facts it calls `search_by_id("1015")` â†’ gets the blob â†’ sends the ENTIRE blob to Gemini
- Gemini sees an unstructured paragraph and must guess which field is which
- If the CNN predicted a coin type that's outside the 438 (or a truly unknown coin), the KB returns nothing

**AFTER the upgrade â€” what the KB will become:**
- All 9,716 CN types (one-time scrape, ~2.7 hours) â€” the KB now covers the FULL domain
- Each coin split into 5 focused chunks: `identity`, `obverse`, `reverse`, `material`, `context`
- 9,716 Ã— 5 = 48,580 vectors in ChromaDB (~180 MB on disk)
- Hybrid search: BM25 keyword search + vector semantic search, merged with RRF formula
- Historian injects each chunk as a labeled `[CONTEXT N]` block â†’ Gemini can only state facts from the context â†’ zero hallucination
- Investigator searches ALL 9,716 types (no filter) â†’ unknown coins now surface real matches
- `in_training_set: bool` tag on every record â†’ easy to see if a match is CNN-known or KB-only

**Known gaps (to fix in enterprise upgrade):**
1. Only 438 types â†’ should be ALL 9,716
2. One blob per coin â†’ should be 5 semantic chunks
3. Vector-only search â†’ no BM25, no hybrid, no RRF
4. `in_training_set` tag MISSING from `build_metadata_dict()`

---

### PHASE 7 â€” All 5 Agents (February 2026) âœ… WORKING

End-to-end test passing: type 1015, 91.1% confidence, historian route, PDF generated.

**The 5 agents and what each one does:**

| File | Role | Input | Output |
|------|------|-------|--------|
| `gatekeeper.py` | Orchestrator â€” runs the LangGraph state machine, routes by confidence | image path | final state dict |
| `historian.py` | Pulls KB facts + calls Gemini to write historical narrative | CNN prediction dict | narrative, mint, date, material... |
| `investigator.py` | For unknown coins â€” sends image to Gemini Vision, extracts visual attributes, cross-refs KB | image path | visual description, detected features, KB matches |
| `validator.py` | OpenCV forensic check â€” detects gold/silver/bronze from HSV pixel analysis, compares to expected material | image path + CNN prediction | match/mismatch, warning |
| `synthesis.py` | Assembles ALL agent outputs into one structured plain-text summary and a professional PDF | full CoinState dict | PDF file + text report |

See **Section 6 (Layer-by-Layer)** for exact per-agent code details.

---

### PHASE 8 â€” Bug Fixing Marathon (February 2026) âœ…

All bugs fully documented in **Section 11 (Known Bugs)**.

---

### PHASE 9 â€” End-to-End Test (February 2026) âœ…

**File:** `scripts/test_pipeline.py`

```
Input:    data/processed/1015/any_coin.jpg
CNN:      type 1015, 91.1% confidence
Route:    historian
KB:       found â€” Maroneia, Thrace, c.365-330 BC, silver drachm
LLM:      narrative generated (GITHUB_TOKEN) or fallback (no key)
PDF:      written to reports/
Exit:     0
```

Latest clean commit: `113514b` â€” Greek transliteration + footer band fix.
Persistent context file committed: `ca96c10`.

---

### PHASE 10 â€” Enterprise RAG Upgrade (February 27, 2026) âœ… COMPLETE

This phase transformed the system from a 438-type demo into a production-grade pipeline covering 97.7% more of the CN numismatic domain.

**STEP 0 â€” Expand the scraper to all 9,716 types**

File: `scripts/build_knowledge_base.py` â€” added `--all-types` flag.

The original scraper only fetched the 438 CNN training types. The KB is pure text â€” it has NO image constraint â€” so there is no reason to limit it to the CNN training set.

Scrape stats:
```
9,716 type IDs targeted
9,541 successfully scraped (175 returned HTTP errors during the run)
Output: data/metadata/cn_types_metadata_full.json  (~3.2 MB)
Speed: 1 req/sec (rate-limited to respect corpus-nummorum.eu)
Duration: ~2h 41min
Resumable: --resume flag skips already-fetched IDs
```

Bug found and fixed during this step:
- ETA formula displayed "~161h 56min" instead of "~2h 41min" (see Bug 11)

Commit: `0abf192`

---

**STEP 1 â€” Build `src/core/rag_engine.py`**

New file: `src/core/rag_engine.py` (674 lines)

**WHY a new file instead of extending knowledge_base.py:**
The old KB was a thin ChromaDB wrapper. The RAG engine is a different beast â€” it needs BM25 index management, RRF score merging, per-chunk metadata, and a `get_context_blocks()` method that returns 5 structured blocks. Mixing these concerns would make knowledge_base.py unmaintainable. The old KB is kept as a fallback reference.

**Architecture:**
```python
class RAGEngine:
    # WHAT: BM25 keyword index + ChromaDB vector index + RRF merger
    # WHY BM25: "silver" matches all silver coins exactly â€” vector search alone
    #           can miss exact keyword hits when the embedding moves words around
    # WHY RRF: No cross-encoder model needed for 9,716 records;
    #           score(d) = sum(1 / (60 + rank_r(d))) gives ~95% of reranker accuracy
    #           at zero latency overhead

    def search(query, n, where=None)       # hybrid BM25+vector+RRF
    def get_by_id(type_id)                 # exact type lookup
    def get_context_blocks(type_id)        # returns 5 labeled [CONTEXT N] strings
    def populate_chroma()                  # one-time build (called by rebuild_chroma.py)
    def is_chroma_built()                  # check before rebuild
    def corpus_size()                      # returns record count
```

**5 Semantic Chunks per coin type:**
```
chunk_type="identity"  â†’ type_id, denomination, authority, region, date_range
chunk_type="obverse"   â†’ obverse description + legend
chunk_type="reverse"   â†’ reverse description + legend
chunk_type="material"  â†’ material, weight, diameter, mint
chunk_type="context"   â†’ persons, references, notes
```

Smoke test result: `9,541 records loaded, 47,705 chunks prepared, BM25 working`

Commit: `514d674`

---

**STEP 2 â€” Rebuild ChromaDB index**

New script: `scripts/rebuild_chroma.py`

Old DB: `data/metadata/chroma_db/` â€” 434 vectors (1 blob per type, 438 types)
New DB: `data/metadata/chroma_db_rag/` â€” 47,705 vectors (5 chunks per type, 9,541 types)

```
Vectors: 47,705 / 47,705 (100%)
Duration: 9.0 minutes
Batch size: 500 (ChromaDB upsert limit)
Speed: 11.3 ms/chunk
On-disk size: ~180 MB
```

The old DB is preserved at `chroma_db/` for fallback. The new DB at `chroma_db_rag/` is the production index.

Commit: `0ef040c` (same as STEP 3)

---

**STEP 3 â€” Upgrade `historian.py` to true RAG**

File: `src/agents/historian.py`

Before STEP 3, the historian did:
```
get_by_id("1015") â†’ ONE 200-word blob â†’ pasted into Gemini prompt â†’ Gemini guesses field structure
```

After STEP 3, it does:
```
get_by_id("1015") â†’ RAGEngine.get_context_blocks("1015") â†’ 5 labeled blocks â†’ grounded prompt:
  [CONTEXT 1 â€” Identity]   denomination: drachm | region: Thrace | date: c.365â€“330 BC
  [CONTEXT 2 â€” Obverse]    bunch of grapes on vine branch | legend MAR
  [CONTEXT 3 â€” Reverse]    legend EPI ZINONOS
  [CONTEXT 4 â€” Material]   silver | weight: 2.44g | mint: Maroneia
  [CONTEXT 5 â€” Context]    persons: Magistrate Zenon

  INSTRUCTION: Using ONLY the contexts above (cite [CONTEXT N]),
               write a 3-paragraph professional analysis.
               Do not add any fact not present in the context.
```

Result: zero hallucination on structured facts, LLM only contributes prose quality.

**Critical bug found and fixed during STEP 3 (see Bug 12):**
`class_id` is the raw softmax output index (0 to 437), NOT the CN type ID. Using `class_id` directly to call `get_by_id()` would look up year index 0 instead of type 1015. Must use `label_str` (the folder name, e.g. `"1015"`).

Before fix: researcher for coin 1015 returned type 5045 (wrong dynasty, wrong region).
After fix: returns correct Maroneia drachm.

Commit: `0ef040c`

---

**STEP 4 â€” Upgrade `investigator.py`**

File: `src/agents/investigator.py`

Two changes:
1. **KB cross-reference scope**: switched from `self._kb.search()` (434 types) to `self._rag.search()` (9,541 types). A low-confidence coin may match a type outside the CNN training set.
2. **OpenCV fallback when no vision LLM key is available:**

```python
def _opencv_fallback(self, image_path: str) -> tuple[str, dict]:
    """
    Pure local analysis when no vision API key is set or the model is not downloaded.

    WHAT: Runs two independent OpenCV analyses:
      1. HSV histogram on 3 crop sizes (40%/60%/80%) with majority vote
         â†’ gold: H 15-35, S 80-255 | bronze: H 5-25, S 50-180 | silver: S < 40
      2. Sobel edge density (gradient magnitude > 30 threshold)
         â†’ higher density = better preserved / more detail visible

    WHY: The system must NEVER return an empty analysis. If qwen3-vl:4b is not
    downloaded, a pure-Python OpenCV fallback still extracts useful attributes
    (metal estimate + condition estimate) that the RAG search can use as a query.
    """
```

Test result: `"silver/gold coin... well-preserved (Sobel 84.2)"`

Commit: `0cfe540`

---

**STEP 5 â€” Upgrade `validator.py`**

File: `src/agents/validator.py`

Three changes:
1. **`label_str` fix** â€” same issue as historian. Was using `class_id` (0-437) for KB lookup. Fixed to use `label_str` (CN type ID string).
2. **Multi-scale HSV**: 3 crop sizes (40 % / 60 % / 80 % of coin center) run independently; majority vote determines the winning metal; single-scale was unreliable on coins with worn edges.
3. **`detection_confidence` + `uncertainty`**:
   - `detection_confidence` (float 0.0â€“1.0): mean pixel coverage of the winning metal mask across all scales that agree
   - `uncertainty`: `"low"` (3/3 scales agree) | `"medium"` (2/3) | `"high"` (1/3 â€” effectively unknown)

Test for route 2 (conf=42.9%): `status=consistent  det_conf=0.73  uncertainty=low`

Commit: `3a82ba2`

---

**STEP 6 â€” Upgrade `gatekeeper.py`**

File: `src/agents/gatekeeper.py` (grew from 245 to 330 lines)

Four engineering improvements:

**1. Structured logging** (`logging.getLogger(__name__)` replaces all bare `print()`):
- Every node emits key metrics at INFO level: label, confidence, route decision, elapsed time, result summary
- `logging.basicConfig()` in `__init__` â€” no-op if caller already configured logging (FastAPI will)
- PDF errors now logged with `exc_info=True` â€” full stack trace captured, not lost to stdout

**2. Per-node timing** (`time.perf_counter()`):
- Each node writes its elapsed seconds into `state["node_timings"][node_name]`
- `analyze()` logs a summary: `total=20.86s  timings={'cnn': '0.54s', 'historian': '19.85s', 'synthesis': '0.47s'}`
- `node_timings: dict` added to `CoinState` TypedDict
- Now we know exactly which node is slow (historian LLM call = 14â€“20s)

**3. Retry with exponential backoff** (`_retry_call(fn, retries=2, backoff=1.5)`):
- Wraps historian and investigator LLM calls
- Retries on 429 (rate limit) and 503 (service unavailable)
- Detects via `exc.status_code` (openai SDK) OR string matching on the error message
- Backoff: 1.5s â†’ 3.0s between retries
- Rationale: >95% of transient 429 errors resolve within 5 seconds

**4. Graceful per-node degradation** (`try/except Exception`):
- Each node catches all exceptions, writes `{"_error": str(exc)}` into the result dict
- The pipeline continues to synthesis â€” which includes the error in the report instead of crashing
- CNN node: NOT wrapped (a CNN failure means no prediction at all â€” surfacing the error is correct)
- All other nodes: fully protected

Bug fixed during this step (see Bug 13): PDF error was printed with bare `print()`. Now `logger.error(exc_info=True)`.

Commit: `3bc9d05`

---

**STEP 7 â€” End-to-end test all 3 routes**

File: `scripts/test_pipeline.py` (completely rewritten from single-route to 3-route test)

Test images discovered by scanning 40 random classes:
- Route 1 (historian, > 85%): `data/processed/1015/CN_type_1015_cn_coin_5943_p.jpg` â€” always type 1015, ~91%
- Route 2 (validator, 40-85%): `data/processed/21027/CN_type_21027_cn_coin_6169_p.jpg` â€” conf 42.9%
- Route 3 (investigator, < 40%): `data/processed/544/CN_type_544_cn_coin_2324_p.jpg` â€” conf 21.3%

Results:
```
Route 1 â€” HISTORIAN   : type=1015  conf=91.1%  time=15.4s   PDF saved   [PASS]
Route 2 â€” VALIDATOR   : label=12884 conf=42.9%  material=consistent  det_conf=0.73  uncertainty=low  time=9.8s   PDF saved   [PASS]
Route 3 â€” INVESTIGATOR: label=532   conf=21.3%  KB matches=3  llm_used=False (OpenCV fallback, qwen3-vl:4b not downloaded)  time=3.1s   PDF saved   [PASS]

RESULTS: 3/3 passed â€” all routes OK    EXIT: 0
```

Test exit code: 0 (clean). `sys.exit(1)` fires only if any assertion fails.

Commit: `9622f66`

---

**STEP 8 â€” Commit, push, update persistent context**

All changes pushed to `ChaiebDhia/DeepCoin-Core` branch `main`. Persistent context file updated.

Latest commit: `5a12ed1` â€” copilot-instructions.md update

---

### CURRENT STATUS â€” Enterprise Layer 3 Upgrade âœ… COMPLETE (all 8 steps done)

**All 8 steps done. Layer 3 is enterprise-grade and production-ready.**
All 3 routing paths tested: historian (91.1%), validator (42.9%), investigator (21.3%) â€” 3/3 PASS.
Next: Layer 4 â€” FastAPI backend.
See **Section 7 (Enterprise Upgrade Plan)** for the full build order record.

---

## 3. ARCHITECTURE â€” TWO-STAGE HYBRID PIPELINE

### Stage 1 â€” Deep Learning (Visual Classification)
```
Raw coin photo
  â†’ CLAHE Enhancement (LAB color space, L-channel only, clipLimit=2.0, tile=8Ã—8)
  â†’ Aspect-preserving resize to 299Ã—299 with zero-padding (no stretching)
  â†’ EfficientNet-B3 (12M params, ImageNet pretrained, fine-tuned on 438 coin classes)
  â†’ 1536-dimensional feature vector
  â†’ Softmax â†’ top-1 class + confidence score + top-5 predictions
```

### Stage 2 â€” Agentic System (Historical Reasoning)
```
confidence > 0.85   â†’  Historian Agent (high confidence â€” RAG + LLM narrative)
0.40 â‰¤ conf â‰¤ 0.85  â†’  Validator Agent + Historian Agent (verify material first)
confidence < 0.40   â†’  Investigator Agent (VLM + local CV fallback â€” unknown coin)

All paths â†’ Synthesis Agent â†’ PDF report
```

### Agent Communication
All agents share a single LangGraph `CoinState` TypedDict:
```python
class CoinState(TypedDict, total=False):
    image_path         : str
    use_tta            : bool
    cnn_prediction     : dict   # {class_id, label, confidence, top5}
    route_taken        : Literal["historian", "validator", "investigator"]
    historian_result   : dict
    validator_result   : dict
    investigator_result: dict
    report             : str    # final Markdown
    pdf_path           : Optional[str]
```

---

### Why CNN AND KB â€” They Cannot Replace Each Other

This question will come from the encadrant: *"If you scrape 9,716 types from the KB, why did you train a CNN? Why not just use the scraper?"*

**Answer: The CNN and KB solve completely different problems. Neither can do the other's job.**

| | CNN | Knowledge Base (KB) |
|---|---|---|
| **Input** | Raw pixel photograph | Text query or coin type_id |
| **Output** | "This looks like type 1015" (visual identity) | "Type 1015 is a silver drachm from Maroneia, 365â€“330 BC" (factual knowledge) |
| **What it learns** | Visual patterns â€” portrait style, iconography, metal texture, patina, geometric proportions | Nothing â€” it is a lookup table with semantic search on top |
| **Can it analyse a photo?** | Yes â€” that is its entire purpose | No â€” it has no vision, only text |
| **Can it explain history?** | No â€” it outputs a class index (e.g. `438`) | Yes â€” it stores the full structured record |
| **Generalises to unseen coins?** | Yes â€” extracts 1536-dim features, returns most visually similar known type | No â€” if type_id is not in the KB, it returns nothing |

**The scraping is data collection. The CNN is pattern recognition. The KB is the encyclopedia. RAG is the retrieval engine.**

A library full of books does not replace a librarian who can look at an artefact and say "this belongs on shelf 7." A librarian who knows which shelf it is on cannot write the book's contents from scratch.

---

### What Happens With Unknown Coins â€” 3 Cases

#### Case A â€” CNN trained on it, KB has it (438 CNN classes)
```
CNN: "type 1015, 91% confidence"
Route: Historian
KB: returns type 1015 record (mint, date, material, obverse, reverse, persons...)
RAG: retrieves 5 focused chunks â†’ injects as [CONTEXT N] blocks â†’ Gemini writes grounded narrative
Report: full professional PDF with historical analysis, forensic check, and visual attributes
```

#### Case B â€” CNN never trained on it, but KB has it (types 439â€“9,716 after upgrade)
```
CNN: misidentifies it as the closest visual match, but confidence is low (< 40%)
Route: Investigator (low confidence triggers VLM path)
Gemini Vision: analyses the photo â†’ "silver coin, helmeted portrait right, legend Î‘ÎÎ¤Î™ÎŸÎ§ÎŸÎ¥, eagle reverse"
KB search (full 9,716 corpus): finds CN type 7432 â€” Seleucid tetradrachm of Antiochos I
Report says: "CNN could not classify this coin (not in training set).
             Visual analysis matched CN type 7432 from knowledge base.
             Confidence: KB match only â€” not CNN-verified."
```
This case transforms from a failure into a success specifically because the KB covers all 9,716 types.

#### Case C â€” Not in CNN, not in KB (completely unknown coin)
```
CNN: low confidence, Investigator route
Gemini Vision: still describes the coin â€” metal, portrait type, legend fragments, symbols
KB search: returns the 3 closest cultural neighbours (similar dynasty, region, period)
Report says: "No exact match in Corpus Nummorum. Closest neighbours: [3 types listed].
             Visual attributes detected: silver, laureate portrait, eagle reverse, possible Greek legend."
```
The system never returns "I don't know." It always returns maximum useful information. This is the *graceful degradation* principle built into the architecture.

---

### What RAG Does â€” The Three-Word Summary: "Makes Gemini Cite Its Sources"

**Without RAG (today):**
```
KB returns one 200-word blob â†’ pasted into Gemini prompt â†’ Gemini writes a paragraph
Problem: Gemini can misread fields, mix up obverse/reverse, or invent plausible-sounding facts
         because it sees unstructured text with no enforcement
```

**With RAG (after upgrade):**
```
KB returns 5 focused chunks (identity, obverse, reverse, material, context)
â†’ Each chunk injected as a labeled block:
    [CONTEXT 1 â€” Identity]  type: 1015 | denom: drachm | region: Thrace | date: 365-330 BC
    [CONTEXT 2 â€” Obverse]   prancing horse right | legend: MAR
    [CONTEXT 3 â€” Reverse]   bunch of grapes | legend: EPI ZINONOS
    [CONTEXT 4 â€” Material]  silver | weight: 2.44 g | mint: Maroneia
    [CONTEXT 5 â€” Context]   persons: Magistrate Zenon
â†’ Strict prompt instruction: "Using ONLY the contexts above (cite [CONTEXT N]),
   write a 3-paragraph analysis. Do not add any fact not present in the context."
â†’ Gemini writes a grounded, citable narrative
```

RAG = **R**etrieve the right chunks â†’ **A**ugment the prompt with them â†’ **G**enerate from those facts only.
The LLM is used for natural language writing quality, not for inventing historical knowledge.

---

## 4. COMPLETE TECHNOLOGY STACK

### Deep Learning
| Component | Version | Detail |
|-----------|---------|--------|
| PyTorch | 2.6.0+cu124 | Neural network framework |
| torchvision | 0.21+ | EfficientNet-B3 pretrained weights |
| EfficientNet-B3 | ImageNet pretrained | 12M params, 1536-dim features, 438-class output head |
| OpenCV | 4.13.0 | CLAHE preprocessing, HSV material detection |
| Albumentations | 1.4+ | Training augmentation pipeline |
| NumPy | 2.x | Numerical ops |
| scikit-learn | latest | Stratified splits, WeightedRandomSampler support |

### Agentic AI
| Component | Version | Detail |
|-----------|---------|--------|
| LangGraph | 0.3+ | State machine orchestration â€” conditional routing, cycles |
| LangChain | 0.3+ | Agent tooling, prompt management |
| openai SDK | latest | Used for BOTH GitHub Models AND Google AI Studio (both OpenAI-compatible) |
| ChromaDB | 0.6+ | Local vector database, persisted to disk |
| sentence-transformers | 3.3+ | `all-MiniLM-L6-v2` embedding model (384-dim, 22MB, CPU) |
| fpdf2 | latest | PDF generation â€” all direct draw primitives, NO Markdown parsing |
| rank-bm25 | latest | BM25Okapi keyword search (to be added in enterprise RAG upgrade) |

### LLM Provider Chain (priority order)
```
1. GITHUB_TOKEN env var  â†’ GitHub Models API (Gemini 2.5 Flash)
   base_url: https://models.inference.ai.azure.com
   model: "gemini-2.5-flash"
   Free with GitHub Copilot Pro student

2. GOOGLE_API_KEY env var â†’ Google AI Studio
   base_url: https://generativelanguage.googleapis.com/v1beta/openai/
   model: "gemini-2.5-flash"
   Free tier: 1,500 req/day

3. OLLAMA_HOST env var â†’ Local Ollama (gemma3:4b or llama3.2:3b)
   Hook written, Ollama NOT currently installed
   gemma3:4b fits in 4.3 GB VRAM

4. None set â†’ structured fallback (KB fields concatenated, no hallucination, no crash)
```

### Backend (Layer 4 â€” pending)
- FastAPI 0.115+ (async, auto-docs, Pydantic v2 validation)
- Uvicorn 0.40+
- SQLAlchemy 2.x async + Alembic migrations
- PostgreSQL 17

### Frontend (Layer 5 â€” pending)
- Next.js 15 (App Router, Server Components)
- TypeScript 5
- Tailwind CSS 4
- shadcn/ui (Radix UI)
- TanStack Query 5
- Zustand 4

### Infrastructure (Layer 6 â€” pending)
- Docker Compose 2.x (7 services)
- Redis 7 (result cache)
- Nginx 1.27 (reverse proxy)
- LocalStack 3.x (AWS S3 + Lambda simulation)
- GitHub Actions (CI: pytest + flake8 + black)

---

## 5. CNN MODEL â€” FULL DETAILS

### Architecture
- **Model**: EfficientNet-B3 (compound scaling: balanced depth/width/resolution)
- **Why B3 not B7**: B7 exceeds 4.3 GB VRAM budget; B3 is optimal param/accuracy ratio
- **Input**: 299Ã—299 RGB
- **Feature extractor output**: 1536-dim vector
- **Output head**: `nn.Linear(1536, 438)` + Dropout(0.4) â€” replaced from original 1000-class head
- **Pretrained on**: ImageNet (1.2M images, 1000 classes)
- **Fine-tuned on**: 438 CN coin types, 7,677 images

### Training Configuration (V3 â€” `scripts/train.py`, 729 lines)
```python
optimizer     = AdamW(lr=1e-4, weight_decay=0.01)
scheduler     = CosineAnnealingLR(T_max=100, eta_min=1e-6)
loss          = CrossEntropyLoss(label_smoothing=0.1)
augmentation  = Albumentations (rotate Â±15Â°, brightness Â±20%, elastic, GaussNoise)
mixup         = alpha=0.2   # blends 2 images: Î»Ã—imgA + (1-Î»)Ã—imgB â€” prevents memorization
amp           = torch.amp.GradScaler('cuda') + autocast  # halves VRAM, ~2Ã— faster
gradient_clip = max_norm=1.0
batch_size    = 16  # GPU memory constraint (4.3 GB VRAM)
early_stop    = patience=10 on val accuracy
pin_memory    = True
non_blocking  = True
seed          = 42
```

### Data Pipeline
```
115,160 raw images (9,716 unique coin types from Corpus Nummorum)
    â†“ filter: â‰¥10 images per class
438 viable classes, 7,677 images
    â†“ CLAHE in LAB color space
    â†“ Aspect-preserving resize to 299Ã—299
    â†“ Stratified 70/15/15 split (seed=42)
    â†“ WeightedRandomSampler (fixes 40:1 class imbalance)
Training: 5,374 images | Validation: 1,151 | Test: 1,152
```

### Results
| Metric | Value |
|--------|-------|
| Best epoch | 52 / 100 |
| Val accuracy (epoch 52) | 79.25% |
| Test accuracy (single pass) | 79.08% |
| **Test accuracy (TTA Ã—8)** | **80.03%** |
| Mean F1 (macro, 438 classes) | 0.7763 |
| Top confusion pair | 3314 â†’ 3987 (10Ã— misclassification) |
| Training duration | ~103 min on RTX 3050 Ti |

### TTA (Test-Time Augmentation)
- 8 passes: original + horizontal flip + 2Ã—vertical variants + 4Ã—crops
- Predictions averaged â†’ +0.78% accuracy gain over single-pass
- Implemented in `src/core/inference.py` â†’ `CoinInference.predict(tta=True)`

### Saved Artefacts
```
models/best_model.pth           # V3 weights (epoch 52) â€” the real model
models/best_model_v1_80pct.pth  # MISLEADING NAME â€” actually epoch 3, val 21.33%, NOT the 80% model
models/class_mapping.pth        # {class_to_idx: {"1015": 0, ...}, idx_to_class: {0: "1015", ...}, n: 438}
```

---

## 6. LAYER-BY-LAYER STATUS

### Layer 0 â€” CNN Training âœ… COMPLETE
File: `scripts/train.py` (729 lines)
Status: EfficientNet-B3 trained, 80.03% TTA accuracy achieved.

### Layer 1 â€” Inference Engine âœ… COMPLETE
Files: `src/core/inference.py`, `scripts/predict.py`
- `CoinInference`: loads model once, runs TTA, returns structured prediction dict
- Device resolution: `"auto"` resolved to `"cuda"` or `"cpu"` before PyTorch sees it
- Bug fixed: original code passed `"auto"` directly to `.to(device)` â†’ RuntimeError

### Layer 2 â€” Knowledge Base âœ… UPGRADED TO FULL CORPUS
Files: `src/core/knowledge_base.py` (legacy fallback), `src/core/rag_engine.py` (production), `scripts/build_knowledge_base.py`, `scripts/rebuild_chroma.py`

**Final state:**
- `knowledge_base.py`: original 434-vector DB kept at `data/metadata/chroma_db/` â€” used as fallback only
- `rag_engine.py`: 47,705-vector DB at `data/metadata/chroma_db_rag/` â€” 9,541 types Ã— 5 semantic chunks
- Hybrid search: BM25Okapi keyword index + ChromaDB vector similarity + RRF (k=60) merge
- `get_context_blocks(type_id)` â†’ returns 5 labeled `[CONTEXT N]` strings ready to inject into LLM prompt
- `in_training_set: bool` tag on every chunk record
- Rebuild script: `scripts/rebuild_chroma.py` (wipe-safe, 9.0 min, 11.3 ms/chunk)

### Layer 3 â€” Agent System âœ… ENTERPRISE UPGRADE COMPLETE
All 5 agents fully upgraded. All 3 routes tested and passing.

**Latest commit**: `9622f66` â€” STEP 7 test_pipeline.py, 3/3 routes PASS

#### Agent Files and Current State:

**`src/agents/gatekeeper.py`** (330 lines) â€” LangGraph orchestrator âœ… UPGRADED
- `CoinState` TypedDict: 11 fields (added `node_timings: dict`)
- `Gatekeeper.__init__()`: `logging.basicConfig()` call + `logger.info()` on init and ready events
- `Gatekeeper.analyze()`: logs entry + pipeline-complete summary with per-node timing dict
- `_build_graph()`: exposes `_retry_call(fn, retries=2, backoff=1.5)` â€” 1.5s/3.0s backoff on 429/503
- Each node: `time.perf_counter()` start/stop, `logger.info()` with key metrics, `try/except` graceful degradation
- `synthesis_node`: PDF error logged with `exc_info=True` instead of bare `print()`
- Routing thresholds: > 0.85 â†’ historian | 0.40â€“0.85 â†’ validator | < 0.40 â†’ investigator

**`src/agents/historian.py`** â€” RAG + LLM narrative âœ… UPGRADED
- `_get_llm(capability)`: separate `_text_client`/`_vision_client` caches â€” 4-provider chain (GitHub/Google/Ollama/fallback)
- `research(cnn_prediction)â†’dict`: `label_str` lookup (NOT raw class_id), `get_by_id()` â†’ hybrid RAG search, `get_context_blocks()` for [CONTEXT 1-5] injection
- `_generate_narrative()`: grounded prompt â€” Gemini cites [CONTEXT N], `max_tokens=800`
- `_fallback_narrative()`: field concatenation when no LLM key

**`src/agents/investigator.py`** â€” VLM visual agent âœ… UPGRADED
- KB cross-reference via `self._rag.search()` â€” all 9,541 types (not just 438)
- `_opencv_fallback()`: HSV histogram (3 crop sizes) â†’ metal detection; Sobel edge density â†’ condition estimate; used when no vision LLM available
- `qwen3-vl:4b` not downloaded yet â†’ fallback always active; pull later: `ollama pull qwen3-vl:4b`

**`src/agents/validator.py`** â€” OpenCV forensic material validator âœ… UPGRADED
- Multi-scale HSV: 40%/60%/80% crop sizes, majority vote on gold/bronze/silver detection
- `detection_confidence` (float 0-1): mean pixel coverage of winning metal across agreeing scales
- `uncertainty` flag: low (3/3 agree) / medium (2/3) / high (1/3)
- `label_str` lookup fix (same as historian â€” NOT raw class_id)

**`src/agents/synthesis.py`** â€” Professional PDF generator âœ… COMPLETE, NO CHANGES NEEDED
- `synthesize(state)â†’str`: clean plain-text summary
- `to_pdf(state, output_path)`: ALL direct fpdf2 draw â€” NO Markdown parsing
- Navy header band, bordered tables with alternating shading, blue section rule lines
- `_GREEK_MAP`: dict-based Greekâ†’Latin transliteration (Îšâ†’K, Î•â†’E, Î¡â†’R, etc.)
- Bug fixed: Greek `???` chars replaced via transliteration map
- Bug fixed: duplicate footer band removed (header already carries branding)
- Signature change from `to_pdf(markdown_str, path)` â†’ `to_pdf(state_dict, path)`

### Layer 4 â€” FastAPI Backend ğŸ”² NEXT (Layer 3 enterprise upgrade complete)
Files to create: `src/api/main.py`, `src/api/routes/classify.py`, `src/api/routes/history.py`, `src/api/schemas.py`
Endpoints planned: `POST /api/classify`, `GET /api/health`, `GET /api/history`, `GET /api/history/{id}`, `WS /ws/classify/{session_id}`

### Layer 5 â€” Next.js Frontend ğŸ”² PENDING
Directory: `frontend/`
Stack: Next.js 15 App Router, TypeScript 5, Tailwind CSS 4, shadcn/ui, TanStack Query 5, Zustand 4

### Layer 6 â€” Docker + Infrastructure ğŸ”² PENDING
File: `docker-compose.yml` (skeleton exists)
7 services: FastAPI + Next.js + ChromaDB + PostgreSQL + Redis + Nginx + LocalStack

### Layer 7 â€” Tests + CI/CD ğŸ”² PENDING
Directories: `tests/unit/`, `tests/integration/`
Stack: pytest 8.x, Jest, Playwright, GitHub Actions (`.github/workflows/ci.yml`)

---

## 7. THE ENTERPRISE UPGRADE PLAN (CURRENT ACTIVE WORK)

This is the work happening NOW before moving to Layer 4.

### The Problem Statement
Current state covers only 4.5% of the CN numismatic domain (438 / 9,716 types). This is the core gap to fix.

### Full 9,716-Type KB Strategy (APPROVED)
- CNN training was limited to 438 types (image threshold â‰¥10 per class)
- KB is pure text â€” has NO image constraint â€” should cover all 9,716 types
- `in_training_set: bool` tag distinguishes CNN-known from KB-only types
- Impact: Investigator transforms from "fallback agent" into "numismatic detective"
- Scrape cost: ~2.7 hours at 1 req/sec (one-time, resumable with `--resume`)

### 5 Semantic Chunks Per Coin
Each coin record split into 5 ChromaDB documents with tagged `chunk_type`:
```
chunk_type="identity"  â†’ type_id, denomination, authority, region, date_range
chunk_type="obverse"   â†’ obverse description + legend
chunk_type="reverse"   â†’ reverse description + legend
chunk_type="material"  â†’ material, weight, diameter, mint
chunk_type="context"   â†’ persons, references, notes
```
Result: 9,716 Ã— 5 = 48,580 vectors (~180 MB ChromaDB on disk)
Why: Each chunk embeds cleanly; "silver coin" search hits material chunks, "eagle reverse" hits reverse chunks.

### Hybrid Search Architecture
```
Query â†’ BM25 keyword search (rank-bm25) â†’ ranked list A
      â†’ ChromaDB vector search            â†’ ranked list B
      â†’ RRF merge: score(d) = Î£ 1/(60 + rank_r(d))
      â†’ final re-ranked list
```
No cross-encoder model (overkill for 9,716 records; RRF gives ~95% of accuracy at 0ms overhead).

### Per-Agent Search Scope
```python
historian()    â†’ hybrid_search(query, where={"type_id": known_id})   # exact type + neighbors
validator()    â†’ hybrid_search(query, where={"chunk_type": "material"})  # material-scoped
investigator() â†’ hybrid_search(query)  # FULL CORPUS â€” no filter, maximum coverage
```

### Grounded LLM Prompt Pattern
```
[CONTEXT 1 â€” Identity] denomination: denarius | authority: Augustus | date: 27 BCâ€“14 AD
[CONTEXT 2 â€” Obverse]  laureate head right | legend: CAESAR AVGVSTVS
[CONTEXT 3 â€” Reverse]  Caius and Lucius standing | legend: PRINCIP IVVENTVTIS
[CONTEXT 4 â€” Material] silver | weight: 3.9g | mint: Lugdunum
[CONTEXT 5 â€” Context]  persons: Augustus, Caius Caesar, Lucius Caesar

INSTRUCTION: You are an expert numismatist. Using ONLY the context above (cite [CONTEXT N]),
write a 3-paragraph professional analysis. Do not add facts not present in the context.
```
This pattern = zero hallucination on structured facts, LLM only adds interpretation.

### Build Order (strict dependency sequence)
```
âœ… STEP 0: Expand build_knowledge_base.py â†’ --all-types flag (scrape 9,716)
         Code complete + smoke test passed. Full scrape running (~2h 42min).
         Output: data/metadata/cn_types_metadata_full.json
         Bug fixed: ETA formula (divided by 60 twice â€” now divides by 3600 for hours)
âœ… STEP 1: Build src/core/rag_engine.py (NEW FILE â€” hybrid search foundation)
         Code complete + smoke test passed. 6,876 records, 34,380 chunks, BM25 working.
         Commit: 514d674
ğŸ”² STEP 2: Rebuild ChromaDB index (5 chunks Ã— 9,716 types = 48,580 vectors)
ğŸ”² STEP 3: Upgrade historian.py (true RAG + "Related Types" section)
ğŸ”² STEP 4: Upgrade investigator.py (full KB search + local CV fallback)
ğŸ”² STEP 5: Upgrade validator.py (confidence scoring + multi-scale HSV)
ğŸ”² STEP 6: Upgrade gatekeeper.py (logging + retry + graceful degradation)
ğŸ”² STEP 7: End-to-end test all 3 routes
ğŸ”² STEP 8: Commit and push
```

---

## 8. KEY ENGINEERING DECISIONS (with rationale)

| Decision | Choice | Why |
|----------|--------|-----|
| CNN architecture | EfficientNet-B3 | Compound scaling; B7 exceeds 4.3 GB VRAM |
| Preprocessing | CLAHE in LAB space | Enhances contrast without destroying metal patina colors |
| Resize strategy | Aspect-preserving + zero-padding | Preserves coin geometry |
| Agent framework | LangGraph (not CrewAI) | Conditional routing + cycles + human-in-loop |
| LLM provider | GitHub Models primary | Free with Copilot Pro student |
| Vector DB | ChromaDB | Local, embeddable, zero network dependency |
| Reranking | RRF score-based (not cross-encoder) | 9,716 records â€” math > extra 65MB model |
| Chunking | 5 semantic chunks per coin | Better embedding precision than 1 blob |
| Architecture style | Modular Monolith | 1-person PFE team; microservices = premature |
| KB scope | All 9,716 types | CNN and KB have independent constraints |
| Ollama | Hook ready, skip install for now | Progressive enhancement |
| Transfer learning norm | [0.485, 0.456, 0.406] / [0.229, 0.224, 0.225] | ImageNet stats â€” mandatory for pretrained weights |
| Augmentation | Albumentations pipeline | 6Ã— synthetic expansion from 7,677 images |
| Class imbalance | WeightedRandomSampler (1/class_count) | Fixes 40:1 imbalance between most/least common types |

---

## 9. FILE STRUCTURE (complete)

```
C:\Users\Administrator\deepcoin\
â”‚
â”œâ”€â”€ .github/
â”‚   â”œâ”€â”€ copilot-instructions.md   â† THIS FILE â€” persistent context
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci.yml                â† Layer 7 (pending)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_pipeline/
â”‚   â”‚   â””â”€â”€ prep_engine.py        âœ… CLAHE + aspect-preserving resize
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ model_factory.py      âœ… EfficientNet-B3 definition (Dropout=0.4)
â”‚   â”‚   â”œâ”€â”€ dataset.py            âœ… DeepCoinDataset + Albumentations transforms
â”‚   â”‚   â”œâ”€â”€ inference.py          âœ… CoinInference (TTA, device auto-resolve)
â”‚   â”‚   â”œâ”€â”€ knowledge_base.py     âœ… ChromaDB wrapper (438 types, kept for fallback)
â”‚   â”‚   â””â”€â”€ rag_engine.py         âœ… hybrid BM25+vector+RRF search engine â€” 47,705 vectors
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ gatekeeper.py         âœ… LangGraph orchestrator â€” logging, timing, retry, degradation
â”‚   â”‚   â”œâ”€â”€ historian.py          âœ… true RAG + [CONTEXT N] citation + Ollama provider
â”‚   â”‚   â”œâ”€â”€ investigator.py       âœ… RAG 9,541 types + OpenCV fallback
â”‚   â”‚   â”œâ”€â”€ validator.py          âœ… multi-scale HSV, detection_confidence, uncertainty
â”‚   â”‚   â””â”€â”€ synthesis.py          âœ… PDF generator â€” COMPLETE, no changes needed
â”‚   â””â”€â”€ api/
â”‚       â”œâ”€â”€ main.py               ğŸ”² FastAPI entry point (Layer 4)
â”‚       â”œâ”€â”€ routes/
â”‚       â”‚   â”œâ”€â”€ classify.py       ğŸ”² POST /api/classify (Layer 4)
â”‚       â”‚   â””â”€â”€ history.py        ğŸ”² GET /api/history (Layer 4)
â”‚       â””â”€â”€ schemas.py            ğŸ”² Pydantic models (Layer 4)
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py                  âœ… CNN training V3 (729 lines, AMP+Mixup)
â”‚   â”œâ”€â”€ audit.py                  âœ… F1 + confusion matrix evaluation
â”‚   â”œâ”€â”€ evaluate_tta.py           âœ… TTA evaluation (+0.78% = 80.03%)
â”‚   â”œâ”€â”€ predict.py                âœ… CLI inference tool
â”‚   â”œâ”€â”€ test_pipeline.py          âœ… End-to-end test (type 1015, all 3 routes)
â”‚   â”œâ”€â”€ test_dataset.py           âœ… Dataset validation
â”‚   â””â”€â”€ build_knowledge_base.py   âœ… Web scraper + ChromaDB builder â€” NEEDS --all-types flag
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ best_model.pth            âœ… V3 weights â€” epoch 52, val 79.25%, test 79.08%, TTA 80.03%
â”‚   â”œâ”€â”€ best_model_v1_80pct.pth   âš ï¸  MISLEADING NAME â€” epoch 3, val 21.33%, NOT 80%
â”‚   â””â”€â”€ class_mapping.pth         âœ… {class_to_idx, idx_to_class, n=438}
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ processed/                âœ… 7,677 images Ã— 438 classes (299Ã—299 JPEG)
â”‚   â”œâ”€â”€ metadata/
â”‚   â”‚   â”œâ”€â”€ cn_types_metadata.json âœ… 515 KB â€” 438 types (needs expansion to 9,716)
â”‚   â”‚   â””â”€â”€ chroma_db/            âœ… ChromaDB persisted â€” 434 vectors (needs rebuild)
â”‚   â””â”€â”€ raw/                      âš ï¸  Original 115k images â€” gitignored, may be on disk
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/                     ğŸ”² Layer 7
â”‚   â””â”€â”€ integration/              ğŸ”² Layer 7
â”‚
â”œâ”€â”€ frontend/                     ğŸ”² Next.js 15 (Layer 5)
â”œâ”€â”€ notebooks/                    exploration
â”œâ”€â”€ reports/                      PDF output directory
â”‚
â”œâ”€â”€ requirements.txt              âœ… All Python dependencies (50+ packages)
â”œâ”€â”€ docker-compose.yml            ğŸ”² 7-service skeleton (Layer 6)
â”œâ”€â”€ .env                          âš ï¸  Secrets file â€” gitignored, NEVER commit
â”‚                                    Contains: GITHUB_TOKEN, GOOGLE_API_KEY
â””â”€â”€ .gitignore                    âœ… Excludes: data/, models/, venv/, .env, notes.md

```

---

## 10. ENVIRONMENT AND PATHS

```powershell
# Activate venv (always do this first)
& C:\Users\Administrator\deepcoin\venv\Scripts\Activate.ps1

# Working directory
C:\Users\Administrator\deepcoin\

# Python 3.11 in venv
C:\Users\Administrator\deepcoin\venv\Scripts\python.exe

# Key installed packages (selected)
torch==2.6.0+cu124
torchvision==0.21.0+cu124
efficientnet-pytorch (via torchvision models)
opencv-python==4.13.0
albumentations==1.4+
chromadb==0.6+
sentence-transformers==3.3+
langgraph==0.3+
langchain==0.3+
openai (latest)
fpdf2 (latest)
scikit-learn (latest)
tqdm
rank-bm25           # installed (STEP 1 â€” RAG engine BM25 index)
ollama (0.17.4)     # for local LLM inference (gemma3:4b downloaded)
# qwen3-vl:4b      # NOT yet downloaded; pull when needed: ollama pull qwen3-vl:4b
```

---

## 11. COMMIT HISTORY (significant milestones)

| Commit | Description |
|--------|-------------|
| Initial commits | Phase 0: project setup, venv, gitignore, README |
| â€” | Phase 1: CLAHE preprocessing pipeline, 7,677 images |
| â€” | Phase 3 (Dataset): DeepCoinDataset + Albumentations |
| â€” | Phase 4 (Training V3): AMP + Mixup + WeightedSampler |
| â€” | Phase 2 (KB): ChromaDB build, 434 docs |
| â€” | Layer 3 agents: all 5 written |
| â€” | Bug fixes: IndentationError historian, device 'auto' gatekeeper, multi_cell synthesis |
| â€” | PDF redesign: direct fpdf2 draw (navy header, bordered tables, no Markdown parsing) |
| `113514b` | Greek transliteration fix + duplicate footer band removal |
| `0abf192` | STEP 0: build_knowledge_base.py --all-types, 9,541 scraped, resume bug fix |
| `514d674` | STEP 1: src/core/rag_engine.py â€” BM25+vector+RRF, 47,705 chunks |
| `0ef040c` | STEP 2+3: ChromaDB rebuilt 47,705 vectors; historian.py true RAG + label_str fix |
| `0cfe540` | STEP 4: investigator.py â€” RAG search 9,541 types + OpenCV fallback |
| `3a82ba2` | STEP 5: validator.py â€” multi-scale HSV, detection_confidence, uncertainty |
| `3bc9d05` | STEP 6: gatekeeper.py â€” logging, per-node timing, retry, graceful degradation |
| `9622f66` | STEP 7+8: test_pipeline.py 3/3 routes PASS + git push â† LATEST |

---

## 12. KNOWN BUGS AND RESOLVED BUGS

---

### FULLY RESOLVED BUGS âœ…

#### Bug 1 â€” `IndentationError` in `historian.py`
- **When:** First test run of historian agent
- **Symptom:** `IndentationError: unexpected indent` at startup
- **Root cause:** A leftover `pass` / TODO stub inside a method body was deleted, leaving orphaned indentation on the next line
- **Fix:** Cleaned the method body â€” removed the stub, completed the method properly

---

#### Bug 2 â€” `RuntimeError: Invalid device string 'auto'`
- **File:** `src/agents/gatekeeper.py` â†’ propagated from device config
- **When:** First time running the full pipeline with `device="auto"`
- **Symptom:** `RuntimeError: Invalid device string: 'auto'` from PyTorch
- **Root cause:** `"auto"` was passed directly as a device string to `CoinInference(device="auto")` â†’ PyTorch only accepts `"cuda"`, `"cpu"`, `"cuda:0"` etc.
- **Fix:** Added device resolution before instantiation:
```python
if device == "auto":
    device = "cuda" if torch.cuda.is_available() else "cpu"
```

---

#### Bug 3 â€” `multi_cell` horizontal position drift in `synthesis.py`
- **When:** Rendering bordered tables in the PDF
- **Symptom:** Table cells overflowed page margins; text ran off the right edge
- **Root cause:** fpdf2's `multi_cell()` does NOT preserve the X cursor. After each cell, the cursor drifted right. Subsequent `multi_cell()` calls started at the wrong X position.
- **Fix:** Added `pdf.set_x(col_x)` immediately before every `multi_cell()` call to restore correct column position.

---

#### Bug 4 â€” Greek characters rendered as `???` in PDF
- **File:** `src/agents/synthesis.py`
- **When:** Rendering coins with Greek legends (e.g., `ÎšÎ•Î¡`, `ÎœÎ‘Î¡`, `Î£Î‘Î¤`)
- **Symptom:** All Greek Unicode characters replaced by `?` in the PDF output
- **Root cause:** fpdf2's built-in fonts (Helvetica/Arial) use Latin-1 encoding. Python's `str.encode("latin-1")` replaces any character outside the Latin-1 range (U+0100+) with `?`. Greek alphabet is U+0370â€“U+03FF â€” entirely outside Latin-1.
- **Fix:** Added `_GREEK_MAP` dict (48 characters â€” full uppercase + lowercase Greek â†’ Latin) and `_s(text)` wrapper function. **Every** text string passed to fpdf2 goes through `_s()` first:
```python
_GREEK_MAP = {"Î‘":"A","Î’":"B","Î“":"G","Î”":"D","Î•":"E","Î–":"Z","Î—":"E",
              "Î˜":"TH","Î™":"I","Îš":"K","Î›":"L","Îœ":"M","Î":"N",
              "Î":"X","ÎŸ":"O","Î ":"P","Î¡":"R","Î£":"S","Î¤":"T",
              "Î¥":"Y","Î¦":"PH","Î§":"CH","Î¨":"PS","Î©":"O", ...}

def _s(text: str) -> str:
    """Transliterate Greek, then encode to latin-1 safely."""
    for gr, lat in _GREEK_MAP.items():
        text = text.replace(gr, lat)
    return text.encode("latin-1", "replace").decode("latin-1")
```

---

#### Bug 5 â€” Extra blank page with branding footer
- **File:** `src/agents/synthesis.py`
- **When:** Any coin analysis that fills almost a full PDF page
- **Symptom:** PDF had an extra blank page at the end with only the navy branding band
- **Root cause:** `_draw_footer_band()` was called unconditionally at the end of `to_pdf()`. If the content had already filled the previous page to capacity, fpdf2 automatically opened a new page before rendering the footer band.
- **Fix:** Removed `_draw_footer_band()` call entirely (the navy header band already carries branding). Footer was purely cosmetic and caused layout corruption.

---

#### Bug 6 â€” `to_pdf()` signature mismatch between Synthesis and Gatekeeper
- **Files:** `src/agents/synthesis.py` (changed), `src/agents/gatekeeper.py` (also needed update)
- **When:** PDF redesign refactor (replacing Markdown parsing with direct fpdf2 draw)
- **Symptom:** `TypeError: to_pdf() takes 2 positional arguments but 3 were given`
- **Root cause:** `synthesis.py` was refactored:
  - Old: `to_pdf(markdown_str: str, path: str)` â€” took the text report as input
  - New: `to_pdf(state: dict, path: str)` â€” takes the full CoinState dict directly
  But `gatekeeper.py` was still calling the old signature: `synthesis.to_pdf(state["report"], pdf_path)`
- **Fix:** Updated `synthesis_node` inside `gatekeeper.py`:
```python
# Old (broken):
synthesis.to_pdf(state.get("report", ""), pdf_path)
# New (correct):
synthesis.to_pdf(state, pdf_path)
```

---

#### Bugs 7-10 â€” Scraper bugs in `build_knowledge_base.py`

**Bug 7 â€” SSL certificate error:**
- **Symptom:** `ssl.SSLCertVerificationError` when fetching corpus-nummorum.eu in lab environment
- **Root cause:** Corporate/lab network intercepts HTTPS â€” certificate chain validation fails
- **Fix:** `ssl.create_default_context()` with `check_hostname=False, verify_mode=ssl.CERT_NONE`

**Bug 8 â€” Emoji/navigation garbage in scraped text:**
- **Symptom:** Metadata fields contained chars like `ğŸ”ââœ¤` from website navigation icons
- **Root cause:** BeautifulSoup extracts ALL text from `<dl>` elements including icon characters
- **Fix:** `re.sub(r"[^\x00-\x7F\u00C0-\u024F\u0370-\u03FF]", "", s)` in `_clean()` function â€” strips non-Latin/non-Greek Unicode from all scraped text

**Bug 9 â€” Mint field "Region:" contamination:**
- **Symptom:** `mint = "Maroneia  Region: Thrace  Typology: Type Group X"`
- **Root cause:** HTML `<dl>` for Mint sometimes contained the Region and Typology sub-labels inline with the value
- **Fix:**
```python
mint_parts = re.split(r"\s+Region:", raw_mint)
mint = mint_parts[0].strip()
region = re.sub(r"\s+Typology.*", "", mint_parts[1]).strip() if len(mint_parts) > 1 else ""
```

**Bug 10 â€” 4 types returned HTTP errors:**
- **Symptom:** After scraping 438 types, only 434 documents appeared in ChromaDB
- **Root cause:** 4 type IDs in `class_mapping.pth` returned 404/500 from corpus-nummorum.eu (likely types removed from the database since the dataset was published)
- **Fix:** `build_from_metadata()` filters error records:
```python
records = [r for r in metadata if "error" not in r]
```

---

#### Bug 11 â€” ETA printed as "~161h 56min" instead of "~2h 41min"
- **File:** `scripts/build_knowledge_base.py` â†’ `main()` ETA block
- **When:** First full `--all-types` run (9,716 types). ETA line read "~161h 56min at 1 req/sec".
- **Root cause:** The formula divided by 60 once, treating the result as hours:
  ```python
  eta_min = len(class_ids) // 60   # 9716 // 60 = 161 â† this is MINUTES, not hours
  eta_sec = len(class_ids) % 60
  print(f"~{eta_min}h {eta_sec:02d}min")  # printed 161h 56min â† WRONG
  ```
  At 1 req/sec, 9,716 requests = 9,716 **seconds** total. Correct conversion needs `// 3600` for hours.
- **Fix:**
  ```python
  _total_s  = len(class_ids)           # seconds at 1 req/sec
  eta_hours = _total_s // 3600         # 9716 // 3600 = 2
  eta_min   = (_total_s % 3600) // 60  # (9716 % 3600) // 60 = 41
  print(f"~{eta_hours}h {eta_min:02d}min at 1 req/sec")  # ~2h 41min â† CORRECT
  ```

---

#### Bug 12 â€” `class_id` is NOT the CN type ID
- **Files:** `src/agents/historian.py`, `src/agents/validator.py`
- **When:** STEP 3 â€” first run of historian with RAG lookup
- **Symptom:** For coin image from class `1015/`, historian returned historical data for type 5045 (a completely different dynasty, region, and period). The coin was Maroneia Thrace but the narrative described a different mint entirely.
- **Root cause:** `cnn_prediction["class_id"]` is the **softmax tensor index** (integer 0â€“437), assigned by `enumerate()` over the alphabetically sorted class folder names. It is NOT the CN type number. The folder `1015/` happens to be at index 0 in the sorted list, so `class_id=0` maps to type 1015. Using that raw integer `0` to call `get_by_id(0)` looked up a completely different type.
  ```python
  # WRONG â€” class_id is 0, 1, 2 ... 437 (sort order position)
  cn_type_id = cnn_prediction["class_id"]       # e.g. 0
  kb_record  = rag.get_by_id(cn_type_id)         # looks up type "0" â€” doesn't exist

  # CORRECT â€” label is the original folder name = CN type ID
  label_str  = cnn_prediction["label"]           # e.g. "1015"
  cn_type_id = int(label_str) if label_str.isdigit() else label_str
  kb_record  = rag.get_by_id(cn_type_id)         # looks up type 1015 âœ”
  ```
- **Fix:** Every agent that needs the CN type ID for KB lookup must use `label_str`, not `class_id`. Applied to `historian.py` (STEP 3) and `validator.py` (STEP 5).

---

#### Bug 13 â€” PDF error silently lost to bare `print()`
- **File:** `src/agents/gatekeeper.py` â€” `synthesis_node`
- **When:** Present from the initial agent implementation; discovered and fixed in STEP 6
- **Symptom:** If PDF rendering raised an exception, the error was printed to stdout with `print(f"[Gatekeeper] PDF error: {_pdf_err}")` + `traceback.print_exc()`. In a production setting (FastAPI server, Docker), stdout may be redirected or suppressed. The error would be silently lost and the caller would only see `pdf_path: null` with no explanation.
- **Root cause:** Early implementation used `print()` as a placeholder during development. Never upgraded to the logging system.
- **Fix:**
  ```python
  # Old (broken):
  except Exception as _pdf_err:
      print(f"[Gatekeeper] PDF error: {_pdf_err}")
      import traceback; traceback.print_exc()
      pdf_path = None

  # New (correct):
  except Exception as pdf_err:
      logger.error("synthesis_node PDF error: %s", pdf_err, exc_info=True)
      pdf_path = None
  ```
  `exc_info=True` captures the full stack trace in the log record, regardless of how the process output is redirected.

---

### KNOWN ISSUES (all resolved in enterprise upgrade)

All Layer 3 enterprise upgrade items are COMPLETE. No remaining scheduled issues.
See Section 7 Build Order for what was fixed and in which commit.
  â†’ Structured fields scraped from corpus-nummorum.eu
  â†’ Validated by Berlin-Brandenburg Academy of Sciences (DFG-funded)
  â†’ Stored in ChromaDB, searched via hybrid BM25+vector

Priority 2: Nomisma.org SPARQL (secondary)
  â†’ Academic linked open data â€” emperor names, reign periods, mint locations
  â†’ RDF structured data, authoritative for numismatic domain

Priority 3: LLM synthesis (tertiary)
  â†’ Gemini 2.5 Flash generates prose from injected context chunks
  â†’ LLM WRITES, it does not INVENT â€” all facts come from [CONTEXT N] blocks

Priority 4: Wikipedia API (last resort)
  â†’ Only for emperor biography narrative when no structured source covers it
  â†’ Always flagged in output: "Source: Wikipedia (unverified)"
```

---

## 14. PERFORMANCE TARGETS

| Metric | Target | Current |
|--------|--------|---------|
| CNN Top-1 accuracy | >85% | 80.03% (TTA) â€” gap ~5pp |
| CNN Top-5 accuracy | >95% | Not measured yet |
| Per-class recall (rare) | >50% | Unknown |
| Full pipeline latency | <20s (LLM) / <3s (no LLM) | Historian: ~15s (Ollama gemma3:4b) / Validator: ~10s / Investigator: ~3s (OpenCV only) |
| PDF generation | <500ms | ~0.40â€“0.47s measured |
| KB search latency | <50ms | Sub-ms (ChromaDB) |

---

## 15. ACADEMIC CONTEXT

- **Institution**: ESPRIT School of Engineering, Manouba, Tunisia
- **Company**: YEBNI â€” Information & Communication, Tunisia (yebni.com)
- **Type**: PFE (Projet de Fin d'Ã‰tudes) â€” 5-month final year internship
- **Period**: February â€“ July 2026
- **Dataset**: Corpus Nummorum v1 â€” 115,160 images, 9,716 types, DFG-funded
- **Problem domain**: Fine-grained archaeological numismatics with long-tail distribution
- **Key contribution**: Hybrid CNN + multi-agent RAG system with graceful degradation for OOD inputs

---

## 16. HOW TO RESUME IN ANY NEW CHAT

1. **This file is already injected.** Copilot knows everything â€” no re-explaining needed.
2. Say: **"Start Layer 4 â€” FastAPI backend."** or **"What is the current status and what should we do next?"**
3. Always activate venv first: `& C:\Users\Administrator\deepcoin\venv\Scripts\Activate.ps1`
4. Iron rule still applies: **discuss plan first â†’ wait for "go" â†’ then build.**
5. All 8 enterprise upgrade steps are done. Layer 3 is production-ready. Layer 4 is next.

```powershell
# Quick health check on resume
& C:\Users\Administrator\deepcoin\venv\Scripts\Activate.ps1
# Verify pipeline still passes
& "C:\Users\Administrator\deepcoin\venv\Scripts\python.exe" scripts/test_pipeline.py 2>$null
# Show exit code
Write-Host "EXIT: $LASTEXITCODE"
```

**Enterprise RAG upgrade: ALL 8 STEPS COMPLETE âœ…**
```
âœ… STEP 0 â€” build_knowledge_base.py --all-types   9,541/9,716 scraped  0abf192
âœ… STEP 1 â€” src/core/rag_engine.py                47,705 chunks        514d674
âœ… STEP 2 â€” ChromaDB rebuilt                      47,705 vectors       0ef040c
âœ… STEP 3 â€” historian.py true RAG                 [CONTEXT N]          0ef040c
âœ… STEP 4 â€” investigator.py upgrade               OpenCV fallback      0cfe540
âœ… STEP 5 â€” validator.py upgrade                  multi-scale HSV      3a82ba2
âœ… STEP 6 â€” gatekeeper.py upgrade                 logging+retry        3bc9d05
âœ… STEP 7 â€” end-to-end test                       3/3 PASS             9622f66
âœ… STEP 8 â€” commit + push                         pushed to GitHub     9622f66
```

**NEXT: Layer 4 â€” FastAPI backend.**
Say: "Start Layer 4 â€” FastAPI backend."
