# DeepCoin-Core â€” Copilot Persistent Context
# ============================================
# This file is automatically injected into every GitHub Copilot Chat session.
# It gives Copilot full knowledge of the project state, decisions, and rules.
# NEVER delete this file. Update it after every major milestone.
# Last updated: February 27, 2026

---

## 0. IRON RULES â€” READ THESE FIRST, NEVER VIOLATE THEM

1. **"Never go to the next layer unless all is engineered as experts will do â€” enterprise-grade and production-ready."**
2. **"Don't add any code unless we discuss it first."** Always present the plan, wait for "go" approval.
3. **"Explain everything like teaching â€” WHAT it does, WHY it's designed this way, HOW it fits."**
4. Every function must have detailed docstrings.
5. This is a PFE (Final Year Engineering Internship) â€” ESPRIT School of Engineering Ã— YEBNI, Tunisia.
6. Student: **Dhia Chaieb** | GitHub: `ChaiebDhia` | Email: `dhia.chaieb@esprit.tn`
7. GitHub repo: `https://github.com/ChaiebDhia/DeepCoin-Core` | Branch: `main`
8. Always assume the Python venv at `C:\Users\Administrator\deepcoin\venv\` is active.
9. OS: Windows 11 | Shell: PowerShell 5.1 | Use `;` not `&&` to chain commands.
10. GPU: NVIDIA RTX 3050 Ti, 4.3 GB VRAM | CUDA 12.4 | PyTorch 2.6.0+cu124

---

## 1. PROJECT MISSION

Build an end-to-end industrial AI system that:
- Classifies degraded archaeological ancient coins from a photograph
- Routes the analysis through specialist AI agents based on confidence
- Returns a professional PDF report with historical narrative, forensic validation, and visual attributes
- Handles unknown coins gracefully (never returns "I don't know" â€” always returns useful output)
- Covers the full Corpus Nummorum domain (9,716 coin types in KB, 438 in CNN)

**The core philosophy:** "Failing gracefully is better than failing confidently."

---

## 2. ARCHITECTURE â€” TWO-STAGE HYBRID PIPELINE

### Stage 1 â€” Deep Learning (Visual Classification)
```
Raw coin photo
  â†’ CLAHE Enhancement (LAB color space, L-channel only, clipLimit=2.0, tile=8Ã—8)
  â†’ Aspect-preserving resize to 299Ã—299 with zero-padding (no stretching)
  â†’ EfficientNet-B3 (12M params, ImageNet pretrained, fine-tuned on 438 coin classes)
  â†’ 1536-dimensional feature vector
  â†’ Softmax â†’ top-1 class + confidence score + top-5 predictions
```

### Stage 2 â€” Agentic System (Historical Reasoning)
```
confidence > 0.85   â†’  Historian Agent (high confidence â€” RAG + LLM narrative)
0.40 â‰¤ conf â‰¤ 0.85  â†’  Validator Agent + Historian Agent (verify material first)
confidence < 0.40   â†’  Investigator Agent (VLM + local CV fallback â€” unknown coin)

All paths â†’ Synthesis Agent â†’ PDF report
```

### Agent Communication
All agents share a single LangGraph `CoinState` TypedDict:
```python
class CoinState(TypedDict, total=False):
    image_path        : str
    use_tta           : bool
    cnn_prediction    : dict   # {class_id, label, confidence, top5}
    route_taken       : Literal["historian", "validator", "investigator"]
    historian_result  : dict
    validator_result  : dict
    investigator_result: dict
    report            : str    # final Markdown
    pdf_path          : Optional[str]
```

---

## 3. COMPLETE TECHNOLOGY STACK

### Deep Learning
| Component | Version | Detail |
|-----------|---------|--------|
| PyTorch | 2.6.0+cu124 | Neural network framework |
| torchvision | 0.21+ | EfficientNet-B3 pretrained weights |
| EfficientNet-B3 | ImageNet pretrained | 12M params, 1536-dim features, 438-class output head |
| OpenCV | 4.13.0 | CLAHE preprocessing, HSV material detection |
| Albumentations | 1.4+ | Training augmentation pipeline |
| NumPy | 2.x | Numerical ops |
| scikit-learn | latest | Stratified splits, WeightedRandomSampler support |

### Agentic AI
| Component | Version | Detail |
|-----------|---------|--------|
| LangGraph | 0.3+ | State machine orchestration â€” conditional routing, cycles |
| LangChain | 0.3+ | Agent tooling, prompt management |
| openai SDK | latest | Used for BOTH GitHub Models AND Google AI Studio (both OpenAI-compatible) |
| ChromaDB | 0.6+ | Local vector database, persisted to disk |
| sentence-transformers | 3.3+ | `all-MiniLM-L6-v2` embedding model (384-dim, 22MB, CPU) |
| fpdf2 | latest | PDF generation â€” all direct draw primitives, NO Markdown parsing |
| rank-bm25 | latest | BM25Okapi keyword search (to be added in enterprise RAG upgrade) |

### LLM Provider Chain (priority order)
```
1. GITHUB_TOKEN env var  â†’ GitHub Models API (Gemini 2.5 Flash)
   base_url: https://models.inference.ai.azure.com
   model: "gemini-2.5-flash"
   Free with GitHub Copilot Pro student

2. GOOGLE_API_KEY env var â†’ Google AI Studio
   base_url: https://generativelanguage.googleapis.com/v1beta/openai/
   model: "gemini-2.5-flash"
   Free tier: 1,500 req/day

3. OLLAMA_HOST env var â†’ Local Ollama (gemma3:4b or llama3.2:3b)
   Hook written, Ollama NOT currently installed
   gemma3:4b fits in 4.3 GB VRAM

4. None set â†’ structured fallback (KB fields concatenated, no hallucination, no crash)
```

### Backend (Layer 4 â€” pending)
- FastAPI 0.115+ (async, auto-docs, Pydantic v2 validation)
- Uvicorn 0.40+
- SQLAlchemy 2.x async + Alembic migrations
- PostgreSQL 17

### Frontend (Layer 5 â€” pending)
- Next.js 15 (App Router, Server Components)
- TypeScript 5
- Tailwind CSS 4
- shadcn/ui (Radix UI)
- TanStack Query 5
- Zustand 4

### Infrastructure (Layer 6 â€” pending)
- Docker Compose 2.x (7 services)
- Redis 7 (result cache)
- Nginx 1.27 (reverse proxy)
- LocalStack 3.x (AWS S3 + Lambda simulation)
- GitHub Actions (CI: pytest + flake8 + black)

---

## 4. CNN MODEL â€” FULL DETAILS

### Architecture
- **Model**: EfficientNet-B3 (compound scaling: balanced depth/width/resolution)
- **Why B3 not B7**: B7 exceeds 4.3 GB VRAM budget; B3 is optimal param/accuracy ratio
- **Input**: 299Ã—299 RGB
- **Feature extractor output**: 1536-dim vector
- **Output head**: `nn.Linear(1536, 438)` + Dropout(0.4) â€” replaced from original 1000-class head
- **Pretrained on**: ImageNet (1.2M images, 1000 classes)
- **Fine-tuned on**: 438 CN coin types, 7,677 images

### Training Configuration (V3 â€” `scripts/train.py`, 729 lines)
```python
optimizer     = AdamW(lr=1e-4, weight_decay=0.01)
scheduler     = CosineAnnealingLR(T_max=100, eta_min=1e-6)
loss          = CrossEntropyLoss(label_smoothing=0.1)
augmentation  = Albumentations (rotate Â±15Â°, brightness Â±20%, elastic, GaussNoise)
mixup         = alpha=0.2   # blends 2 images: Î»Ã—imgA + (1-Î»)Ã—imgB â€” prevents memorization
amp           = torch.amp.GradScaler('cuda') + autocast  # halves VRAM, ~2Ã— faster
gradient_clip = max_norm=1.0
batch_size    = 16  # GPU memory constraint (4.3 GB VRAM)
early_stop    = patience=10 on val accuracy
pin_memory    = True
non_blocking  = True
seed          = 42
```

### Data Pipeline
```
115,160 raw images (9,716 unique coin types from Corpus Nummorum)
    â†“ filter: â‰¥10 images per class
438 viable classes, 7,677 images
    â†“ CLAHE in LAB color space
    â†“ Aspect-preserving resize to 299Ã—299
    â†“ Stratified 70/15/15 split (seed=42)
    â†“ WeightedRandomSampler (fixes 40:1 class imbalance)
Training: 5,374 images | Validation: 1,151 | Test: 1,152
```

### Results
| Metric | Value |
|--------|-------|
| Best epoch | 52 / 100 |
| Val accuracy (epoch 52) | 79.25% |
| Test accuracy (single pass) | 79.08% |
| **Test accuracy (TTA Ã—8)** | **80.03%** |
| Mean F1 (macro, 438 classes) | 0.7763 |
| Top confusion pair | 3314 â†’ 3987 (10Ã— misclassification) |
| Training duration | ~103 min on RTX 3050 Ti |

### TTA (Test-Time Augmentation)
- 8 passes: original + horizontal flip + 2Ã—vertical variants + 4Ã—crops
- Predictions averaged â†’ +0.78% accuracy gain over single-pass
- Implemented in `src/core/inference.py` â†’ `CoinInference.predict(tta=True)`

### Saved Artefacts
```
models/best_model.pth           # V3 weights (epoch 52) â€” the real model
models/best_model_v1_80pct.pth  # MISLEADING NAME â€” actually epoch 3, val 21.33%, NOT the 80% model
models/class_mapping.pth        # {class_to_idx: {"1015": 0, ...}, idx_to_class: {0: "1015", ...}, n: 438}
```

---

## 5. LAYER-BY-LAYER STATUS

### Layer 0 â€” CNN Training âœ… COMPLETE
File: `scripts/train.py` (729 lines)
Status: EfficientNet-B3 trained, 80.03% TTA accuracy achieved.

### Layer 1 â€” Inference Engine âœ… COMPLETE
Files: `src/core/inference.py`, `scripts/predict.py`
- `CoinInference`: loads model once, runs TTA, returns structured prediction dict
- Device resolution: `"auto"` resolved to `"cuda"` or `"cpu"` before PyTorch sees it
- Bug fixed: original code passed `"auto"` directly to `.to(device)` â†’ RuntimeError

### Layer 2 â€” Knowledge Base âœ… COMPLETE (but needs expansion)
Files: `src/core/knowledge_base.py`, `scripts/build_knowledge_base.py`, `data/metadata/cn_types_metadata.json`

**Current state** (needs upgrade):
- ChromaDB collection: `cn_coin_types`, 434 documents
- Embedding: `all-MiniLM-L6-v2` (384-dim, cosine similarity)
- One document per coin = one 200-word text blob per type (BAD â€” to be fixed)
- Only 438 types in KB (BAD â€” should be 9,716)
- `search(query, n, where)` â†’ vector-only search (no BM25)
- `search_by_id(type_id)` â†’ exact ID lookup
- `build_from_metadata(path)` â†’ builds ChromaDB from JSON

**Known critical gaps**:
1. Only 438 types â€” should be ALL 9,716 from Corpus Nummorum
2. One blob per coin â€” should be 5 semantic chunks per type
3. Vector-only search â€” no BM25, no hybrid, no RRF
4. `in_training_set` tag missing (needed to distinguish CNN scope from KB scope)

### Layer 3 â€” Agent System âœ… WORKING â†’ ðŸ”§ ENTERPRISE UPGRADE IN PROGRESS
All 5 agents written, end-to-end test passing (type 1015, 91.1%, historian route, PDF generated).

**Latest commit**: `113514b` â€” Greek transliteration fix + footer band removal

#### Agent Files and Current State:

**`src/agents/gatekeeper.py`** (245 lines) â€” LangGraph orchestrator
- `CoinState` TypedDict: full shared pipeline state
- `Gatekeeper.__init__()`: loads ALL agents once, resolves `"auto"` device
- Routing thresholds: `HIGH_CONF=0.85`, `LOW_CONF=0.40` (class constants)
- Routes: historian / validator+historian / investigator
- **Pending upgrades**: structured logging, retry (up to 2Ã— on 429/503), graceful degradation per node, per-node timing

**`src/agents/historian.py`** (212 lines) â€” RAG + LLM narrative
- `_get_llm()`: GitHub Models / Google AI Studio lazy singleton
- `research(cnn_prediction)â†’dict`: calls `search_by_id()` â†’ passes raw document string to Gemini
- `_generate_narrative(record, confidence)`: single-turn Gemini call
- `_fallback_narrative(record)`: field concatenation when no LLM key
- **Pending upgrades**: true RAG (hybrid search â†’ 5-chunk injection â†’ grounded generation), multi-query retrieval, citation refs, "Related Types" section from full 9,716 KB

**`src/agents/investigator.py`** â€” VLM visual agent
- Base64-encodes image â†’ Gemini Vision 6-point structured prompt
- KB cross-reference: uses Gemini description as semantic search query
- `_parse_features(description)`: naive regex extraction
- **Pending upgrades**: local CV fallback (HSV histogram + Sobel edges + ORB keypoints when no API key), search full 9,716 KB (not just 438), better feature parsing

**`src/agents/validator.py`** â€” OpenCV forensic material validator
- Crops centre 60% of coin, HSV mask analysis
- Gold threshold: H 15-35, S 80-255 | Bronze: H 5-25, S 50-180 | Silver: S < 40
- 15% pixel fraction threshold (hardcoded)
- `_materials_match()`: simplistic string comparison
- **Pending upgrades**: multi-scale (40%/60%/80% crops), confidence score 0-100%, uncertainty flag (low/medium/high), per-channel std analysis, cross-reference KB on mismatch

**`src/agents/synthesis.py`** â€” Professional PDF generator âœ… COMPLETE, NO CHANGES NEEDED
- `synthesize(state)â†’str`: clean plain-text summary
- `to_pdf(state, output_path)`: ALL direct fpdf2 draw â€” NO Markdown parsing
- Navy header band, bordered tables with alternating shading, blue section rule lines
- `_GREEK_MAP`: dict-based Greekâ†’Latin transliteration (Îšâ†’K, Î•â†’E, Î¡â†’R, etc.)
- Bug fixed: Greek `???` chars replaced via transliteration map
- Bug fixed: duplicate footer band removed (header already carries branding)
- Signature change from `to_pdf(markdown_str, path)` â†’ `to_pdf(state_dict, path)`

### Layer 4 â€” FastAPI Backend ðŸ”² PENDING
Files to create: `src/api/main.py`, `src/api/routes/classify.py`, `src/api/routes/history.py`, `src/api/schemas.py`
Endpoints planned: `POST /api/classify`, `GET /api/health`, `GET /api/history`, `GET /api/history/{id}`, `WS /ws/classify/{session_id}`

### Layer 5 â€” Next.js Frontend ðŸ”² PENDING
Directory: `frontend/`
Stack: Next.js 15 App Router, TypeScript 5, Tailwind CSS 4, shadcn/ui, TanStack Query 5, Zustand 4

### Layer 6 â€” Docker + Infrastructure ðŸ”² PENDING
File: `docker-compose.yml` (skeleton exists)
7 services: FastAPI + Next.js + ChromaDB + PostgreSQL + Redis + Nginx + LocalStack

### Layer 7 â€” Tests + CI/CD ðŸ”² PENDING
Directories: `tests/unit/`, `tests/integration/`
Stack: pytest 8.x, Jest, Playwright, GitHub Actions (`.github/workflows/ci.yml`)

---

## 6. THE ENTERPRISE UPGRADE PLAN (CURRENT ACTIVE WORK)

This is the work happening NOW before moving to Layer 4.

### The Problem Statement
Current state covers only 4.5% of the CN numismatic domain (438 / 9,716 types). This is the core gap to fix.

### Full 9,716-Type KB Strategy (APPROVED)
- CNN training was limited to 438 types (image threshold â‰¥10 per class)
- KB is pure text â€” has NO image constraint â€” should cover all 9,716 types
- `in_training_set: bool` tag distinguishes CNN-known from KB-only types
- Impact: Investigator transforms from "fallback agent" into "numismatic detective"
- Scrape cost: ~2.7 hours at 1 req/sec (one-time, resumable with `--resume`)

### 5 Semantic Chunks Per Coin
Each coin record split into 5 ChromaDB documents with tagged `chunk_type`:
```
chunk_type="identity"  â†’ type_id, denomination, authority, region, date_range
chunk_type="obverse"   â†’ obverse description + legend
chunk_type="reverse"   â†’ reverse description + legend
chunk_type="material"  â†’ material, weight, diameter, mint
chunk_type="context"   â†’ persons, references, notes
```
Result: 9,716 Ã— 5 = 48,580 vectors (~180 MB ChromaDB on disk)
Why: Each chunk embeds cleanly; "silver coin" search hits material chunks, "eagle reverse" hits reverse chunks.

### Hybrid Search Architecture
```
Query â†’ BM25 keyword search (rank-bm25) â†’ ranked list A
      â†’ ChromaDB vector search            â†’ ranked list B
      â†’ RRF merge: score(d) = Î£ 1/(60 + rank_r(d))
      â†’ final re-ranked list
```
No cross-encoder model (overkill for 9,716 records; RRF gives ~95% of accuracy at 0ms overhead).

### Per-Agent Search Scope
```python
historian()    â†’ hybrid_search(query, where={"type_id": known_id})   # exact type + neighbors
validator()    â†’ hybrid_search(query, where={"chunk_type": "material"})  # material-scoped
investigator() â†’ hybrid_search(query)  # FULL CORPUS â€” no filter, maximum coverage
```

### Grounded LLM Prompt Pattern
```
[CONTEXT 1 â€” Identity] denomination: denarius | authority: Augustus | date: 27 BCâ€“14 AD
[CONTEXT 2 â€” Obverse]  laureate head right | legend: CAESAR AVGVSTVS
[CONTEXT 3 â€” Reverse]  Caius and Lucius standing | legend: PRINCIP IVVENTVTIS
[CONTEXT 4 â€” Material] silver | weight: 3.9g | mint: Lugdunum
[CONTEXT 5 â€” Context]  persons: Augustus, Caius Caesar, Lucius Caesar

INSTRUCTION: You are an expert numismatist. Using ONLY the context above (cite [CONTEXT N]),
write a 3-paragraph professional analysis. Do not add facts not present in the context.
```
This pattern = zero hallucination on structured facts, LLM only adds interpretation.

### Build Order (strict dependency sequence)
```
STEP 0: Expand build_knowledge_base.py â†’ --all-types flag (scrape 9,716)
STEP 1: Build src/core/rag_engine.py (NEW FILE â€” hybrid search foundation)
STEP 2: Rebuild ChromaDB index (5 chunks Ã— 9,716 types = 48,580 vectors)
STEP 3: Upgrade historian.py (true RAG + "Related Types" section)
STEP 4: Upgrade investigator.py (full KB search + local CV fallback)
STEP 5: Upgrade validator.py (confidence scoring + multi-scale HSV)
STEP 6: Upgrade gatekeeper.py (logging + retry + graceful degradation)
STEP 7: End-to-end test all 3 routes
STEP 8: Commit and push
```

---

## 7. KEY ENGINEERING DECISIONS (with rationale)

| Decision | Choice | Why |
|----------|--------|-----|
| CNN architecture | EfficientNet-B3 | Compound scaling; B7 exceeds 4.3 GB VRAM |
| Preprocessing | CLAHE in LAB space | Enhances contrast without destroying metal patina colors |
| Resize strategy | Aspect-preserving + zero-padding | Preserves coin geometry |
| Agent framework | LangGraph (not CrewAI) | Conditional routing + cycles + human-in-loop |
| LLM provider | GitHub Models primary | Free with Copilot Pro student |
| Vector DB | ChromaDB | Local, embeddable, zero network dependency |
| Reranking | RRF score-based (not cross-encoder) | 9,716 records â€” math > extra 65MB model |
| Chunking | 5 semantic chunks per coin | Better embedding precision than 1 blob |
| Architecture style | Modular Monolith | 1-person PFE team; microservices = premature |
| KB scope | All 9,716 types | CNN and KB have independent constraints |
| Ollama | Hook ready, skip install for now | Progressive enhancement |
| Transfer learning norm | [0.485, 0.456, 0.406] / [0.229, 0.224, 0.225] | ImageNet stats â€” mandatory for pretrained weights |
| Augmentation | Albumentations pipeline | 6Ã— synthetic expansion from 7,677 images |
| Class imbalance | WeightedRandomSampler (1/class_count) | Fixes 40:1 imbalance between most/least common types |

---

## 8. FILE STRUCTURE (complete)

```
C:\Users\Administrator\deepcoin\
â”‚
â”œâ”€â”€ .github/
â”‚   â”œâ”€â”€ copilot-instructions.md   â† THIS FILE â€” persistent context
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci.yml                â† Layer 7 (pending)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_pipeline/
â”‚   â”‚   â””â”€â”€ prep_engine.py        âœ… CLAHE + aspect-preserving resize
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ model_factory.py      âœ… EfficientNet-B3 definition (Dropout=0.4)
â”‚   â”‚   â”œâ”€â”€ dataset.py            âœ… DeepCoinDataset + Albumentations transforms
â”‚   â”‚   â”œâ”€â”€ inference.py          âœ… CoinInference (TTA, device auto-resolve)
â”‚   â”‚   â”œâ”€â”€ knowledge_base.py     âœ… ChromaDB wrapper â€” NEEDS UPGRADE (438â†’9716, chunking)
â”‚   â”‚   â””â”€â”€ rag_engine.py         ðŸ”² NEW â€” hybrid BM25+vector+RRF search engine
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ gatekeeper.py         âœ… LangGraph orchestrator â€” NEEDS logging+retry
â”‚   â”‚   â”œâ”€â”€ historian.py          âœ… LLM narrative â€” NEEDS true RAG upgrade
â”‚   â”‚   â”œâ”€â”€ investigator.py       âœ… VLM agent â€” NEEDS local CV fallback + full KB
â”‚   â”‚   â”œâ”€â”€ validator.py          âœ… OpenCV forensics â€” NEEDS confidence score
â”‚   â”‚   â””â”€â”€ synthesis.py          âœ… PDF generator â€” COMPLETE, no changes needed
â”‚   â””â”€â”€ api/
â”‚       â”œâ”€â”€ main.py               ðŸ”² FastAPI entry point (Layer 4)
â”‚       â”œâ”€â”€ routes/
â”‚       â”‚   â”œâ”€â”€ classify.py       ðŸ”² POST /api/classify (Layer 4)
â”‚       â”‚   â””â”€â”€ history.py        ðŸ”² GET /api/history (Layer 4)
â”‚       â””â”€â”€ schemas.py            ðŸ”² Pydantic models (Layer 4)
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py                  âœ… CNN training V3 (729 lines, AMP+Mixup)
â”‚   â”œâ”€â”€ audit.py                  âœ… F1 + confusion matrix evaluation
â”‚   â”œâ”€â”€ evaluate_tta.py           âœ… TTA evaluation (+0.78% = 80.03%)
â”‚   â”œâ”€â”€ predict.py                âœ… CLI inference tool
â”‚   â”œâ”€â”€ test_pipeline.py          âœ… End-to-end test (type 1015, all 3 routes)
â”‚   â”œâ”€â”€ test_dataset.py           âœ… Dataset validation
â”‚   â””â”€â”€ build_knowledge_base.py   âœ… Web scraper + ChromaDB builder â€” NEEDS --all-types flag
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ best_model.pth            âœ… V3 weights â€” epoch 52, val 79.25%, test 79.08%, TTA 80.03%
â”‚   â”œâ”€â”€ best_model_v1_80pct.pth   âš ï¸  MISLEADING NAME â€” epoch 3, val 21.33%, NOT 80%
â”‚   â””â”€â”€ class_mapping.pth         âœ… {class_to_idx, idx_to_class, n=438}
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ processed/                âœ… 7,677 images Ã— 438 classes (299Ã—299 JPEG)
â”‚   â”œâ”€â”€ metadata/
â”‚   â”‚   â”œâ”€â”€ cn_types_metadata.json âœ… 515 KB â€” 438 types (needs expansion to 9,716)
â”‚   â”‚   â””â”€â”€ chroma_db/            âœ… ChromaDB persisted â€” 434 vectors (needs rebuild)
â”‚   â””â”€â”€ raw/                      âš ï¸  Original 115k images â€” gitignored, may be on disk
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/                     ðŸ”² Layer 7
â”‚   â””â”€â”€ integration/              ðŸ”² Layer 7
â”‚
â”œâ”€â”€ frontend/                     ðŸ”² Next.js 15 (Layer 5)
â”œâ”€â”€ notebooks/                    exploration
â”œâ”€â”€ reports/                      PDF output directory
â”‚
â”œâ”€â”€ requirements.txt              âœ… All Python dependencies (50+ packages)
â”œâ”€â”€ docker-compose.yml            ðŸ”² 7-service skeleton (Layer 6)
â”œâ”€â”€ .env                          âš ï¸  Secrets file â€” gitignored, NEVER commit
â”‚                                    Contains: GITHUB_TOKEN, GOOGLE_API_KEY
â””â”€â”€ .gitignore                    âœ… Excludes: data/, models/, venv/, .env, notes.md

```

---

## 9. ENVIRONMENT AND PATHS

```powershell
# Activate venv (always do this first)
& C:\Users\Administrator\deepcoin\venv\Scripts\Activate.ps1

# Working directory
C:\Users\Administrator\deepcoin\

# Python 3.11 in venv
C:\Users\Administrator\deepcoin\venv\Scripts\python.exe

# Key installed packages (selected)
torch==2.6.0+cu124
torchvision==0.21.0+cu124
efficientnet-pytorch (via torchvision models)
opencv-python==4.13.0
albumentations==1.4+
chromadb==0.6+
sentence-transformers==3.3+
langgraph==0.3+
langchain==0.3+
openai (latest)
fpdf2 (latest)
scikit-learn (latest)
tqdm
rank-bm25  â† to be installed during RAG upgrade
```

---

## 10. COMMIT HISTORY (significant milestones)

| Commit | Description |
|--------|-------------|
| Initial commits | Phase 0: project setup, venv, gitignore, README |
| â€” | Phase 1: CLAHE preprocessing pipeline, 7,677 images |
| â€” | Phase 3 (Dataset): DeepCoinDataset + Albumentations |
| â€” | Phase 4 (Training V3): AMP + Mixup + WeightedSampler |
| â€” | Phase 2 (KB): ChromaDB build, 434 docs |
| â€” | Layer 3 agents: all 5 written |
| â€” | Bug fixes: IndentationError historian, device 'auto' gatekeeper, multi_cell synthesis |
| â€” | PDF redesign: direct fpdf2 draw (navy header, bordered tables, no Markdown parsing) |
| `113514b` | Greek transliteration fix + duplicate footer band removal â† LATEST |

---

## 11. KNOWN BUGS AND RESOLVED BUGS

### Resolved âœ…
- `IndentationError` in `historian.py` â€” leftover TODO stub in method
- `RuntimeError: device 'auto'` â€” `"auto"` was passed directly to `model.to(device)` instead of being resolved to `"cuda"` or `"cpu"` first
- `multi_cell` horizontal space error in `synthesis.py` â€” needed `set_x()` before every `multi_cell` call
- Greek `???` characters in PDF â€” Greek Unicode (ÎšÎ•Î¡) was not supported by fpdf2 default font; fixed with `_GREEK_MAP` dict-based transliteration
- Branding footer band appearing on extra page â€” `_draw_footer_band()` call removed (header band already carries branding)
- `to_pdf()` signature mismatch â€” changed from `(markdown_str, path)` to `(state_dict, path)` and updated gatekeeper call accordingly

### Known (to fix in enterprise upgrade)
- `knowledge_base.py`: 1 blob per coin instead of 5 semantic chunks
- `knowledge_base.py`: only 438 types instead of 9,716
- `historian.py`: raw document blob passed to LLM â€” not true RAG
- `investigator.py`: 100% dependent on Gemini Vision â€” no local CV fallback
- `validator.py`: binary match/mismatch â€” no confidence score
- `gatekeeper.py`: `print()` statements instead of `logging` module

---

## 12. DATA SOURCES AND FALLBACK CHAIN

```
Priority 1: CN Dataset metadata (primary)
  â†’ Structured fields scraped from corpus-nummorum.eu
  â†’ Validated by Berlin-Brandenburg Academy of Sciences (DFG-funded)
  â†’ Stored in ChromaDB, searched via hybrid BM25+vector

Priority 2: Nomisma.org SPARQL (secondary)
  â†’ Academic linked open data â€” emperor names, reign periods, mint locations
  â†’ RDF structured data, authoritative for numismatic domain

Priority 3: LLM synthesis (tertiary)
  â†’ Gemini 2.5 Flash generates prose from injected context chunks
  â†’ LLM WRITES, it does not INVENT â€” all facts come from [CONTEXT N] blocks

Priority 4: Wikipedia API (last resort)
  â†’ Only for emperor biography narrative when no structured source covers it
  â†’ Always flagged in output: "Source: Wikipedia (unverified)"
```

---

## 13. PERFORMANCE TARGETS

| Metric | Target | Current |
|--------|--------|---------|
| CNN Top-1 accuracy | >85% | 80.03% (TTA) â€” gap ~5pp |
| CNN Top-5 accuracy | >95% | Not measured yet |
| Per-class recall (rare) | >50% | Unknown |
| Full pipeline latency | <2s | Not measured (agents pending upgrade) |
| PDF generation | <500ms | Approximately met |
| KB search latency | <50ms | Sub-ms (ChromaDB) |

---

## 14. ACADEMIC CONTEXT

- **Institution**: ESPRIT School of Engineering, Manouba, Tunisia
- **Company**: YEBNI â€” Information & Communication, Tunisia (yebni.com)
- **Type**: PFE (Projet de Fin d'Ã‰tudes) â€” 5-month final year internship
- **Period**: February â€“ July 2026
- **Dataset**: Corpus Nummorum v1 â€” 115,160 images, 9,716 types, DFG-funded
- **Problem domain**: Fine-grained archaeological numismatics with long-tail distribution
- **Key contribution**: Hybrid CNN + multi-agent RAG system with graceful degradation for OOD inputs

---

## 15. HOW TO RESUME IN ANY NEW CHAT

1. The file you're reading is automatically injected â€” Copilot already knows everything.
2. Say: **"Continue the enterprise upgrade of Layer 3 â€” we're at STEP [N] of the build order"**
3. Or say: **"What is the current status and what should we do next?"**
4. Always activate venv first: `& C:\Users\Administrator\deepcoin\venv\Scripts\Activate.ps1`
5. The rule is still: **discuss plan first, wait for "go", then build.**
